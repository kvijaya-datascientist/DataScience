{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064f9bdc",
   "metadata": {},
   "source": [
    "# Using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e719f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387c19fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just information don't runn both either setup one\n",
    "#device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"   # for apple GPU\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # for NVIDIA\n",
    "torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a93540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#download and load the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBert\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBert\")\n",
    "\n",
    "sentences = [\"This framework generates embeddings for each input sentence\"]\n",
    "\n",
    "#Tokenize the sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449e78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1142,  8297, 21241,  9712,  4774,  3408,  1116,  1111,  1296,\n",
      "          7758,  5650,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f68a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13])\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7c9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "this\n",
      "framework\n",
      "generates\n",
      "em\n",
      "##bed\n",
      "##ding\n",
      "##s\n",
      "for\n",
      "each\n",
      "input\n",
      "sentence\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for token in encoded_input['input_ids'][0]:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12be96cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1142,  8297, 21241,  9712,  4774,  3408,  1116,  1111,  1296,\n",
      "          7758,  5650,   102]])\n"
     ]
    }
   ],
   "source": [
    "ids = encoded_input['input_ids']\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f151c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "mask = encoded_input['attention_mask']\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be8ea423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2158, -0.1084,  0.1876,  ...,  0.2099,  0.0843,  0.0233],\n",
      "         [-0.3009, -0.1544,  0.3384,  ...,  0.1853,  0.4096,  0.2572],\n",
      "         [ 0.2038,  0.1068,  0.1621,  ..., -0.0106,  0.0322, -0.0404],\n",
      "         ...,\n",
      "         [-0.0062,  0.0316,  0.0100,  ...,  0.1434, -0.1350,  0.2606],\n",
      "         [ 0.3250, -0.0937,  0.0700,  ...,  0.0812, -0.2481,  0.1599],\n",
      "         [ 0.0678,  0.5106, -0.7916,  ..., -0.0949,  0.5474,  0.7905]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(ids, mask)\n",
    "final_layer = output.last_hidden_state\n",
    "\n",
    "print(final_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af9682",
   "metadata": {},
   "source": [
    "# Using BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27174c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6abeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This framework generates embeddings for each input sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f80ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758ceba8fa324d4eab461a2fa57e0a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  7705, 19421,  7861,  8270,  4667,  2015,  2005,  2169,\n",
      "          7953,  6251,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_input1 = tokenizer1(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output1 = model(**encoded_input1)\n",
    "\n",
    "print(encoded_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a674f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2023,  7705, 19421,  7861,  8270,  4667,  2015,  2005,  2169,\n",
      "          7953,  6251,   102]])\n"
     ]
    }
   ],
   "source": [
    "ids1 = encoded_input1['input_ids']\n",
    "print(ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6b3eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ C L S ]\n",
      "t h i s\n",
      "f r a m e w o r k\n",
      "g e n e r a t e s\n",
      "e m\n",
      "# # b e d\n",
      "# # d i n g\n",
      "# # s\n",
      "f o r\n",
      "e a c h\n",
      "i n p u t\n",
      "s e n t e n c e\n",
      "[ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for token in encoded_input1['input_ids'][0]:\n",
    "    print(tokenizer1.decode(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9badbdb",
   "metadata": {},
   "source": [
    "# Vectors Similarity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbcbad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98353806]]\n"
     ]
    }
   ],
   "source": [
    "# Cosine-similarity check\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity_check = cosine_similarity(ids, ids1)\n",
    "\n",
    "print(cosine_similarity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88201881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
