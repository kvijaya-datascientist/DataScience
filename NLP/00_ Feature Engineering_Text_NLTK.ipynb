{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88049636",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "Text data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f2fc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vijaya/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db42999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vijaya/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d37d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vijaya/sw_install/anaconda3/envs/nlp/nltk_data.\n",
      "[nltk_data]     ..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fad4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<vijaya & We can combine all the [ preprocessing 9 methods above and create a / preprocess function that takes in a .txt file and handles all the preprocessing. We print out the tokens, filtered words (after stopword filtering), stemmed words, d on to the model or foand POS, one of which is usually passer further processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "116ff9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vijaya & We can combine all the [ preprocessing 9 methods above and create a / preprocess function that takes in a .txt file and handles all the preprocessing. We print out the tokens, filtered words (after stopword filtering), stemmed words, d on to the model or foand POS, one of which is usually passer further processing.\n"
     ]
    }
   ],
   "source": [
    "#Removing HTML tags\n",
    "html_tag_remover = re.compile('<.*?>')\n",
    "text = re.sub(html_tag_remover, '', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d226e54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vijaya   we can combine all the   preprocessing   methods above and create a   preprocess function that takes in a  txt file and handles all the preprocessing  we print out the tokens  filtered words  after stopword filtering   stemmed words  d on to the model or foand pos  one of which is usually passer further processing \n"
     ]
    }
   ],
   "source": [
    "#Removing non-alphabets(numbers) and convert to lower case\n",
    "text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2078add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vijaya', 'we', 'can', 'combine', 'all', 'the', 'preprocessing', 'methods', 'above', 'and', 'create', 'a', 'preprocess', 'function', 'that', 'takes', 'in', 'a', 'txt', 'file', 'and', 'handles', 'all', 'the', 'preprocessing', 'we', 'print', 'out', 'the', 'tokens', 'filtered', 'words', 'after', 'stopword', 'filtering', 'stemmed', 'words', 'd', 'on', 'to', 'the', 'model', 'or', 'foand', 'pos', 'one', 'of', 'which', 'is', 'usually', 'passer', 'further', 'processing']\n"
     ]
    }
   ],
   "source": [
    "#Performing tokenization\n",
    "words = word_tokenize(text)  # download 'punkt'\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f31c44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vijaya', 'combine', 'preprocessing', 'methods', 'create', 'preprocess', 'function', 'takes', 'txt', 'file', 'handles', 'preprocessing', 'print', 'tokens', 'filtered', 'words', 'stopword', 'filtering', 'stemmed', 'words', 'model', 'foand', 'pos', 'one', 'usually', 'passer', 'processing']\n"
     ]
    }
   ],
   "source": [
    "#Removing stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "words = [word for word in words if word not in stop_words ]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbca6692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vijaya', 'combine', 'preprocessing', 'method', 'create', 'preprocess', 'function', 'take', 'txt', 'file', 'handle', 'preprocessing', 'print', 'token', 'filtered', 'word', 'stopword', 'filtering', 'stemmed', 'word', 'model', 'foand', 'po', 'one', 'usually', 'passer', 'processing']\n"
     ]
    }
   ],
   "source": [
    "#Perform Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [lemmatizer.lemmatize(word) for word in words]  #download 'wordnet'\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8763d0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vijaya', 'combin', 'preprocess', 'method', 'creat', 'preprocess', 'function', 'take', 'txt', 'file', 'handl', 'preprocess', 'print', 'token', 'filter', 'word', 'stopword', 'filter', 'stem', 'word', 'model', 'foand', 'po', 'one', 'usual', 'passer', 'process']\n"
     ]
    }
   ],
   "source": [
    "#Perform Stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "words = [porter_stemmer.stem(word) for word in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f10a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vijaya', 'combin', 'preprocess', 'method', 'creat', 'preprocess', 'function', 'take', 'txt', 'file', 'handl', 'preprocess', 'print', 'token', 'filter', 'word', 'stopword', 'filter', 'stem', 'word', 'model', 'foand', 'pos', 'one', 'usual', 'passer', 'process']\n"
     ]
    }
   ],
   "source": [
    "#Snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "words = [snow_stemmer.stem(word) for word in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b26f58",
   "metadata": {},
   "source": [
    "# Feature Extraction / Vectorization\n",
    "\n",
    "Text-to-Numeric : we have diff approaches\n",
    "\n",
    "1. Bag of words (BoW)\n",
    "\n",
    "2. TF-IDF\n",
    "\n",
    "3. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57cec78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "065b7b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combine</th>\n",
       "      <th>create</th>\n",
       "      <th>file</th>\n",
       "      <th>filtered</th>\n",
       "      <th>filtering</th>\n",
       "      <th>foand</th>\n",
       "      <th>function</th>\n",
       "      <th>handle</th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>...</th>\n",
       "      <th>print</th>\n",
       "      <th>processing</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>stopword</th>\n",
       "      <th>take</th>\n",
       "      <th>token</th>\n",
       "      <th>txt</th>\n",
       "      <th>usually</th>\n",
       "      <th>vijaya</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   combine  create  file  filtered  filtering  foand  function  handle   \n",
       "0        0       0     0         0          0      0         0       0  \\\n",
       "1        1       0     0         0          0      0         0       0   \n",
       "2        0       0     0         0          0      0         0       0   \n",
       "3        0       0     0         0          0      0         0       0   \n",
       "4        0       1     0         0          0      0         0       0   \n",
       "\n",
       "   method  model  ...  print  processing  stemmed  stopword  take  token  txt   \n",
       "0       0      0  ...      0           0        0         0     0      0    0  \\\n",
       "1       0      0  ...      0           0        0         0     0      0    0   \n",
       "2       0      0  ...      0           0        0         0     0      0    0   \n",
       "3       1      0  ...      0           0        0         0     0      0    0   \n",
       "4       0      0  ...      0           0        0         0     0      0    0   \n",
       "\n",
       "   usually  vijaya  word  \n",
       "0        0       1     0  \n",
       "1        0       0     0  \n",
       "2        0       0     0  \n",
       "3        0       0     0  \n",
       "4        0       0     0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW method\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X1 = bow_vectorizer.fit_transform(words)\n",
    "\n",
    "bow_df = pd.DataFrame(X1.A , columns= bow_vectorizer.get_feature_names_out())\n",
    "bow_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1002a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vijaya': 23, 'combine': 0, 'preprocessing': 14, 'method': 8, 'create': 1, 'preprocess': 13, 'function': 6, 'take': 19, 'txt': 21, 'file': 2, 'handle': 7, 'print': 15, 'token': 20, 'filtered': 3, 'word': 24, 'stopword': 18, 'filtering': 4, 'stemmed': 17, 'model': 9, 'foand': 5, 'po': 12, 'one': 10, 'usually': 22, 'passer': 11, 'processing': 16}\n"
     ]
    }
   ],
   "source": [
    "print(bow_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402b717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 23)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 14)\t1\n",
      "  (3, 8)\t1\n",
      "  (4, 1)\t1\n",
      "  (5, 13)\t1\n",
      "  (6, 6)\t1\n",
      "  (7, 19)\t1\n",
      "  (8, 21)\t1\n",
      "  (9, 2)\t1\n",
      "  (10, 7)\t1\n",
      "  (11, 14)\t1\n",
      "  (12, 15)\t1\n",
      "  (13, 20)\t1\n",
      "  (14, 3)\t1\n",
      "  (15, 24)\t1\n",
      "  (16, 18)\t1\n",
      "  (17, 4)\t1\n",
      "  (18, 17)\t1\n",
      "  (19, 24)\t1\n",
      "  (20, 9)\t1\n",
      "  (21, 5)\t1\n",
      "  (22, 12)\t1\n",
      "  (23, 10)\t1\n",
      "  (24, 22)\t1\n",
      "  (25, 11)\t1\n",
      "  (26, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X1) # BoW weights for each terms belong to a particular documen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991d2909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combine</th>\n",
       "      <th>create</th>\n",
       "      <th>file</th>\n",
       "      <th>filtered</th>\n",
       "      <th>filtering</th>\n",
       "      <th>foand</th>\n",
       "      <th>function</th>\n",
       "      <th>handle</th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>...</th>\n",
       "      <th>print</th>\n",
       "      <th>processing</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>stopword</th>\n",
       "      <th>take</th>\n",
       "      <th>token</th>\n",
       "      <th>txt</th>\n",
       "      <th>usually</th>\n",
       "      <th>vijaya</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   combine  create  file  filtered  filtering  foand  function  handle   \n",
       "0      0.0     0.0   0.0       0.0        0.0    0.0       0.0     0.0  \\\n",
       "1      1.0     0.0   0.0       0.0        0.0    0.0       0.0     0.0   \n",
       "\n",
       "   method  model  ...  print  processing  stemmed  stopword  take  token  txt   \n",
       "0     0.0    0.0  ...    0.0         0.0      0.0       0.0   0.0    0.0  0.0  \\\n",
       "1     0.0    0.0  ...    0.0         0.0      0.0       0.0   0.0    0.0  0.0   \n",
       "\n",
       "   usually  vijaya  word  \n",
       "0      0.0     1.0   0.0  \n",
       "1      0.0     0.0   0.0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF method\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(words)\n",
    "\n",
    "tfidf_df = pd.DataFrame(np.round(X.A,3) , columns= tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8501ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vijaya': 23, 'combine': 0, 'preprocessing': 14, 'method': 8, 'create': 1, 'preprocess': 13, 'function': 6, 'take': 19, 'txt': 21, 'file': 2, 'handle': 7, 'print': 15, 'token': 20, 'filtered': 3, 'word': 24, 'stopword': 18, 'filtering': 4, 'stemmed': 17, 'model': 9, 'foand': 5, 'po': 12, 'one': 10, 'usually': 22, 'passer': 11, 'processing': 16}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.vocabulary_) # vocabulary terms and their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "449b550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 23)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 14)\t1.0\n",
      "  (3, 8)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 13)\t1.0\n",
      "  (6, 6)\t1.0\n",
      "  (7, 19)\t1.0\n",
      "  (8, 21)\t1.0\n",
      "  (9, 2)\t1.0\n",
      "  (10, 7)\t1.0\n",
      "  (11, 14)\t1.0\n",
      "  (12, 15)\t1.0\n",
      "  (13, 20)\t1.0\n",
      "  (14, 3)\t1.0\n",
      "  (15, 24)\t1.0\n",
      "  (16, 18)\t1.0\n",
      "  (17, 4)\t1.0\n",
      "  (18, 17)\t1.0\n",
      "  (19, 24)\t1.0\n",
      "  (20, 9)\t1.0\n",
      "  (21, 5)\t1.0\n",
      "  (22, 12)\t1.0\n",
      "  (23, 10)\t1.0\n",
      "  (24, 22)\t1.0\n",
      "  (25, 11)\t1.0\n",
      "  (26, 16)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(X) # tf-idf weights for each terms belong to a particular documen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eafe1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec \n",
    "\n",
    "#Mostly, NLTK corpus/data works very well with 'gensim' models. (i.e) \n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba790863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "print(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3294687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training our data/words with gensim model\n",
    "gensim_model = word2vec.Word2Vec(words, sg=1, workers=cores-1, window=4, min_count=0)\n",
    "\n",
    "#saving the model with name 'gensim_Word2Vec_Model1'\n",
    "gensim_model.save('generated/word2vecmodel.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da0597f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 0,\n",
       " 'o': 1,\n",
       " 'r': 2,\n",
       " 's': 3,\n",
       " 't': 4,\n",
       " 'n': 5,\n",
       " 'p': 6,\n",
       " 'i': 7,\n",
       " 'd': 8,\n",
       " 'a': 9,\n",
       " 'c': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'f': 13,\n",
       " 'g': 14,\n",
       " 'w': 15,\n",
       " 'u': 16,\n",
       " 'h': 17,\n",
       " 'y': 18,\n",
       " 'k': 19,\n",
       " 'b': 20,\n",
       " 'j': 21,\n",
       " 'x': 22,\n",
       " 'v': 23}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9376a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#loading the saved word2vec model\n",
    "gensim_load_model = KeyedVectors.load('generated/word2vecmodel.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a3c1ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.3926115e-04,  2.5880086e-04,  5.1066871e-03, ...,\n",
       "        -7.0486129e-03,  9.0537930e-04,  6.3952692e-03],\n",
       "       [-8.6522466e-03,  3.6946868e-03,  5.1975371e-03, ...,\n",
       "        -2.3710402e-03, -9.5002307e-03,  4.5305295e-03],\n",
       "       [ 1.7434624e-05,  3.1616748e-03, -6.8913666e-03, ...,\n",
       "         5.5341172e-04,  8.2652131e-03, -6.9025490e-03],\n",
       "       ...,\n",
       "       [ 1.2983537e-03,  6.6034454e-03,  9.9805482e-03, ...,\n",
       "         9.0810293e-03, -5.7746135e-03,  3.7277236e-03],\n",
       "       [-2.3433071e-04,  4.2182151e-03,  2.1172029e-03, ...,\n",
       "         7.2509079e-05,  2.6453231e-06,  8.7128431e-03],\n",
       "       [-2.5250332e-03, -5.8976240e-03,  7.4725486e-03, ...,\n",
       "         2.1580623e-03,  6.6877826e-04,  9.5317243e-03]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_load_model.wv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a45c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
